{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyG45Qk3qQLS"
      },
      "source": [
        "# API - Dynamox\n",
        "Upload dass bases de dados Dynamox do GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR921S_OQSHG"
      },
      "source": [
        "## Package install & Update\n",
        "A seguir estão os pacotes requeridos para tratamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "WUtu4316QSHL"
      },
      "outputs": [],
      "source": [
        "%pip install markdown -q\n",
        "%pip install PyGithub -q\n",
        "%pip install gitpython -q\n",
        "%pip install openpyxl -q\n",
        "%pip install requests -q\n",
        "%pip install google-cloud-bigquery -q\n",
        "\n",
        "# Atualizar pacotes\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_TOKEN = \"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\""
      ],
      "metadata": {
        "id": "cLiWUP7o6hCm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Geração do Token e API para download dos dados da plataforma Dynamox"
      ],
      "metadata": {
        "id": "hAt1QCnpk4ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # pip install requests\n",
        "import jwt  # pip install pyjwt\n",
        "import base64\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# URLs e caminho para o JSON e para o GitHub\n",
        "APPLICATION_KEY_URL = \"https://raw.githubusercontent.com/CidClayQuirino/DataBaseDynamox/main/SOTREQ.json\"\n",
        "AUTH_URL = \"https://api.dynamox.solutions/v1/token\"  # Substitua pelo endpoint que retorna o access_token\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "GITHUB_PATH = \"token.json\"  # Caminho no repositório onde o token será salvo\n",
        "\n",
        "# Função para baixar o JSON diretamente da URL\n",
        "def load_json_from_url(url: str) -> dict:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(\"Erro ao baixar o JSON:\", response.status_code)\n",
        "        exit()\n",
        "\n",
        "# Função para gerar o token JWT\n",
        "def generate_token(data: dict) -> str:\n",
        "    return jwt.encode(\n",
        "        headers={\n",
        "            \"kid\": data[\"_id\"],\n",
        "            \"alg\": \"RS256\",\n",
        "            \"typ\": \"JWT\",\n",
        "        },\n",
        "        payload={\n",
        "            \"iat\": datetime.now(timezone.utc).timestamp(),\n",
        "            \"email\": data[\"email\"],\n",
        "        },\n",
        "        key=data[\"privateKey\"].encode(\"utf-8\"),\n",
        "        algorithm=\"RS256\"\n",
        "    )\n",
        "\n",
        "# Função para solicitar o access_token usando o token JWT\n",
        "def get_access_token(jwt_token: str) -> str:\n",
        "    headers = {\"Authorization\": f\"Bearer {jwt_token}\"}\n",
        "    response = requests.post(url=AUTH_URL, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"access_token\")\n",
        "    else:\n",
        "        print(\"Erro ao obter o access_token:\", response.status_code, response.text)\n",
        "        exit()\n",
        "\n",
        "# Função para fazer upload do JSON com o token para o GitHub\n",
        "def upload_to_github(json_content: str, repo: str, path: str, github_token: str):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(json_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": \"Upload do token de autenticação em JSON\",\n",
        "        \"content\": content\n",
        "    }\n",
        "    # Verifica se o arquivo já existe para obter o SHA e fazer o update\n",
        "    get_response = requests.get(url, headers=headers)\n",
        "    if get_response.status_code == 200:\n",
        "        sha = get_response.json()[\"sha\"]\n",
        "        data[\"sha\"] = sha\n",
        "\n",
        "    # Faz o upload ou update do arquivo no GitHub\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(\"Token JSON salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(\"Erro ao salvar o token JSON no GitHub:\", response.status_code, response.text)\n",
        "\n",
        "# Carrega o JSON da URL\n",
        "json_data = load_json_from_url(APPLICATION_KEY_URL)\n",
        "\n",
        "# Gera o token JWT usando os dados do JSON\n",
        "jwt_token = generate_token(json_data)\n",
        "print(\"Token JWT gerado:\", jwt_token)\n",
        "\n",
        "# Obtém o access_token usando o token JWT\n",
        "access_token = get_access_token(jwt_token)\n",
        "print(\"Access token obtido:\", access_token)\n",
        "\n",
        "# Salva o access_token em JSON e faz upload para o GitHub\n",
        "token_data = {\"access_token\": access_token}\n",
        "token_json = json.dumps(token_data, indent=4)\n",
        "upload_to_github(token_json, GITHUB_REPO, GITHUB_PATH, GITHUB_TOKEN)"
      ],
      "metadata": {
        "id": "HfWCKz3nlB_k",
        "outputId": "72220d97-c57b-403b-9d15-3a2d36b8ee5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token JWT gerado: eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3MjhmN2Q4NTZkMTJkNzI2MGVhMzU0MyIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzExNjgxNDMuMjQ4ODE4LCJlbWFpbCI6IjY3MjhmN2JkNWZiYzllM2YzZTE4Yzg1M0BzYS5keW5hbW94LnNvbHV0aW9ucyJ9.kB5MOsLfTfNxDnW_2fEf_u9TpPvAcNWsYQas8ztklNrt-4LiEcecB7bnT9oPhRHCdCmWlBDw4xEA5vOPT3Ep0ns1w9jdMV4y1bs-rItbIv2_xU_KXcy5arL4UBHj1kkXthfvmqGNmG8YBo0hF0IVlCy886TiefiH-HmJ_LuKDjMGc02zNRShBzMjUUSnBRkSUoLwejlHo1A0gh3I5v_x_rnxPKNAE5pqSREx_k_YlJsEpvHW_covLhpvPy1rnQMVRYf9QxAhugDIE-7P4CIb_zrd4yXJkFIFoglHSRDtgFsUsbAnz0YO4UIAbuVPKnP7ZDOukcJsd6qZRxY5eUu3FA\n",
            "Access token obtido: eyJhbGciOiJSUzI1NiIsImtpZCI6ImI4Y2FjOTViNGE1YWNkZTBiOTY1NzJkZWU4YzhjOTVlZWU0OGNjY2QiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiU09UUkVRIFMvQSAtIE1haW4gYXBwbGljYXRpb24iLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vcHJlZGljdC1wcm9kdWN0aW9uLXBsYXRmb3JtIiwiYXVkIjoicHJlZGljdC1wcm9kdWN0aW9uLXBsYXRmb3JtIiwiYXV0aF90aW1lIjoxNzMxMTY4MTQzLCJ1c2VyX2lkIjoia1YyVU5hRE5ROFZYMXJjM0hPYlZnU29pTG11MSIsInN1YiI6ImtWMlVOYUROUThWWDFyYzNIT2JWZ1NvaUxtdTEiLCJpYXQiOjE3MzExNjgxNDMsImV4cCI6MTczMTE3MTc0MywiZW1haWwiOiI2NzI4ZjdiZDVmYmM5ZTNmM2UxOGM4NTNAc2EuZHluYW1veC5zb2x1dGlvbnMiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZW1haWwiOlsiNjcyOGY3YmQ1ZmJjOWUzZjNlMThjODUzQHNhLmR5bmFtb3guc29sdXRpb25zIl19LCJzaWduX2luX3Byb3ZpZGVyIjoicGFzc3dvcmQifX0.Wy6kefLlCiVqOirEco2AEfom7Nk76TU-jqhuEtceYBoaU67K0B95DZAQfYbCZc8EzbXZoff8DnMU6pA9sH-1IGsLH3ox5oTL7QgYvf3YnjSbxLUinLKM7jllvfx6SDj1Umo6PBfK0o_rF_d9FPzyszOUfkCvnTgVBPsg9SmLZXchHkoHLso4zS-J31Q6Kf0sxNZ9OI2S88-apq4PL-7VmNTXObS6r4C1XkZPb8wB4TRJLY_NdoHj5Ke8thg8zGAQhIck-mmPtrEz4rVIC3Rh21XEJAODOvcIlHIVMBxRiocXksh8UTCd-w07JLkjdHVVsjGE9Vr4KYWPrv5MyhtV1w\n",
            "Token JSON salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download dos dados com base nos Spots e Metrics definidos"
      ],
      "metadata": {
        "id": "Wlyh_fPgLy_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurações do GitHub e URLs\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "GITHUB_PATH = \"token.json\"  # Caminho do token.json no repositório\n",
        "CSV_PATH_ON_GITHUB = \"metrics_data.csv\"\n",
        "url_metric_descriptor = \"https://api.dynamox.solutions/v1/telemetry/metric-descriptor\"\n",
        "resource_ids = [\n",
        "    \"668febb5b059577b39e5fa09\", \"668febb2a9700b05a71ca37d\", \"668ff6e6765a3c089d872db8\",\n",
        "    \"668ff6e9a9700bbff21ca91c\", \"668ff96083c61336dd517c5d\", \"66cf825924e3d86b465ec589\",\n",
        "    \"668fe7e938e2532db463d8b6\", \"668fe7eba9700bf2361ca195\", \"66dedf8945d1a4943a5c84c6\",\n",
        "    \"66ddd667ea40c41c2eee6f05\", \"66d70a35a6119ffc0cd30d32\", \"668febadb05957ae16e5fa05\",\n",
        "    \"66ddda0b13eff3dbf760a767\", \"66dedc24efc0bac50c80870f\", \"66dedf28ea40c41c2eef012b\"\n",
        "]\n",
        "\n",
        "# Função para baixar e carregar o token JSON do repositório GitHub\n",
        "def load_github_token(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return json.loads(content)\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Carrega o access_token do token.json no GitHub\n",
        "token_data = load_github_token(GITHUB_REPO, GITHUB_PATH, GITHUB_TOKEN)\n",
        "if token_data:\n",
        "    access_token = token_data.get(\"access_token\")  # Substitua \"access_token\" pela chave exata dentro do token.json\n",
        "    if not access_token:\n",
        "        raise ValueError(\"Access token não encontrado em token.json.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o token JSON do GitHub.\")\n",
        "\n",
        "# Cabeçalho de autorização com o token JWT\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "\n",
        "# Consulta os descritores de métricas para cada resource_id\n",
        "all_metrics = []\n",
        "for resource_id in resource_ids:\n",
        "    params = {\"resourceId\": resource_id}\n",
        "    response = requests.get(url_metric_descriptor, headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        metric_descriptors = response.json()\n",
        "        all_metrics.extend(metric_descriptors)\n",
        "    else:\n",
        "        print(f\"Falha na solicitação para resource_id {resource_id}: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Cria um DataFrame e salva como CSV\n",
        "df = pd.DataFrame(all_metrics)\n",
        "csv_data = df.to_csv(index=False)\n",
        "\n",
        "# Função para fazer upload do CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content: str, repo: str, path: str, github_token: str):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de dados de métricas - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "    # Verifica se o arquivo já existe para atualização\n",
        "    get_response = requests.get(url, headers=headers)\n",
        "    if get_response.status_code == 200:\n",
        "        sha = get_response.json()[\"sha\"]\n",
        "        data[\"sha\"] = sha\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(\"CSV salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o CSV no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Upload do CSV para o GitHub\n",
        "upload_csv_to_github(csv_data, GITHUB_REPO, CSV_PATH_ON_GITHUB, GITHUB_TOKEN)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUXOZqrdAjVf",
        "outputId": "445206db-dd62-4467-a77d-8bfce986b8ff"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ETL: Etapa 1) Criação do df_dynamox para salvar todos os dados por um periodo de tempo"
      ],
      "metadata": {
        "id": "Wc6P-KRwL9jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime\n",
        "import io\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "TOKEN_PATH = \"token.json\"  # Caminho do token.json no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "url_data_points = \"https://api.dynamox.solutions/v1/telemetry/data-points/aggregation\"\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    # Encode the content to base64\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de dados agregados - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo df_dynamox.csv salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carrega o access_token do token.json no GitHub\n",
        "token_data = json.loads(load_github_file(GITHUB_REPO, TOKEN_PATH, GITHUB_TOKEN))\n",
        "if token_data:\n",
        "    token = token_data.get(\"access_token\")\n",
        "    if not token:\n",
        "        raise ValueError(\"Access token não encontrado em token.json.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o token JSON do GitHub.\")\n",
        "\n",
        "# Carregar o arquivo metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Definindo o cabeçalho de autorização com o token\n",
        "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "# Período de consulta\n",
        "from_time = \"2024-01-01\"\n",
        "to_time = \"2024-12-30\"\n",
        "\n",
        "# DataFrame agregado para todas as métricas\n",
        "df_dynamox = pd.DataFrame()\n",
        "\n",
        "# Loop sobre cada metricDescriptorId no DataFrame de métricas\n",
        "for metric_descriptor_id in metrics_df[\"metricDescriptorId\"]:\n",
        "    # Parâmetros da consulta para cada ID de métrica\n",
        "    params = {\n",
        "        \"metricDescriptorId\": metric_descriptor_id,\n",
        "        \"fromTime\": f\"{from_time}T12:33:11.123Z\",\n",
        "        \"toTime\": f\"{to_time}T12:33:11.123Z\",\n",
        "    }\n",
        "\n",
        "    # Fazendo a requisição para a API Dynamox\n",
        "    response = requests.get(url_data_points, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Processando os dados de resposta\n",
        "        data_points = response.json()\n",
        "        df = pd.DataFrame(data_points)\n",
        "\n",
        "        # Expandindo a coluna de agregação\n",
        "        aggregation_expanded = pd.json_normalize(df['aggregation'])\n",
        "\n",
        "        # Concatenando as colunas expandidas ao DataFrame original\n",
        "        df_expanded = pd.concat([df.drop(columns=['aggregation']), aggregation_expanded], axis=1)\n",
        "\n",
        "        # Adicionando uma coluna para identificar o metricDescriptorId\n",
        "        df_expanded['metricDescriptorId'] = metric_descriptor_id\n",
        "\n",
        "        # Agregando ao DataFrame principal\n",
        "        df_dynamox = pd.concat([df_dynamox, df_expanded], ignore_index=True)\n",
        "\n",
        "    else:\n",
        "        print(f\"Falha na solicitação para metricDescriptorId {metric_descriptor_id}: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Após o loop, salvar o DataFrame agregado como um único arquivo CSV\n",
        "output_filename = \"df_dynamox.csv\"\n",
        "csv_content = df_dynamox.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7zDM2zSJxl5",
        "outputId": "dfc7342a-499f-4cbf-a47f-b13a83fce7df"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo df_dynamox.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "import io\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "TOKEN_PATH = \"token.json\"  # Caminho do token.json no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "url_data_points = \"https://api.dynamox.solutions/v1/telemetry/data-points/aggregation\"\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de dados agregados - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo salvo com sucesso no GitHub: {path}\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carrega o access_token do token.json no GitHub\n",
        "token_data = json.loads(load_github_file(GITHUB_REPO, TOKEN_PATH, GITHUB_TOKEN))\n",
        "if token_data:\n",
        "    token = token_data.get(\"access_token\")\n",
        "    if not token:\n",
        "        raise ValueError(\"Access token não encontrado em token.json.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o token JSON do GitHub.\")\n",
        "\n",
        "# Carregar o arquivo metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Definindo o cabeçalho de autorização com o token\n",
        "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "# Definindo as datas de início e término para consulta diária\n",
        "start_date = datetime.strptime(\"2024-09-03\", \"%Y-%m-%d\")\n",
        "end_date = datetime.now()  # Data atual como data final\n",
        "\n",
        "# DataFrame agregado para todas as consultas\n",
        "df_dynamoxAllData = pd.DataFrame()\n",
        "\n",
        "# DataFrame para armazenar a última data consultada com sucesso para cada metric_descriptor_id\n",
        "df_metric_descriptor_id_end_date = pd.DataFrame(columns=[\"metric_descriptor_id\", \"last_end_date\"])\n",
        "\n",
        "# Loop sobre cada metricDescriptorId no DataFrame de métricas\n",
        "for metric_descriptor_id in metrics_df[\"metricDescriptorId\"]:\n",
        "    current_date = start_date\n",
        "    last_successful_date = start_date  # Inicializa a última data bem-sucedida com a data de início\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        # Configuração das datas para consulta diária\n",
        "        from_time = current_date.strftime(\"%Y-%m-%dT00:00:00.000Z\")\n",
        "        to_time = (current_date + timedelta(days=1)).strftime(\"%Y-%m-%dT00:00:00.000Z\")\n",
        "\n",
        "        # Parâmetros da consulta para cada ID de métrica com intervalo diário\n",
        "        params = {\n",
        "            \"metricDescriptorId\": metric_descriptor_id,\n",
        "            \"fromTime\": from_time,\n",
        "            \"toTime\": to_time,\n",
        "        }\n",
        "\n",
        "        # Fazendo a requisição para a API Dynamox\n",
        "        response = requests.get(url_data_points, headers=headers, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Processando os dados de resposta\n",
        "            data_points = response.json()\n",
        "            if data_points:\n",
        "                df = pd.DataFrame(data_points)\n",
        "\n",
        "                # Expandindo a coluna de agregação\n",
        "                aggregation_expanded = pd.json_normalize(df['aggregation'])\n",
        "\n",
        "                # Concatenando as colunas expandidas ao DataFrame original\n",
        "                df_expanded = pd.concat([df.drop(columns=['aggregation']), aggregation_expanded], axis=1)\n",
        "\n",
        "                # Adicionando uma coluna para identificar o metricDescriptorId\n",
        "                df_expanded['metricDescriptorId'] = metric_descriptor_id\n",
        "\n",
        "                # Agregando ao DataFrame principal\n",
        "                df_dynamoxAllData = pd.concat([df_dynamoxAllData, df_expanded], ignore_index=True)\n",
        "\n",
        "            print(f\"Dados agregados para metricDescriptorId {metric_descriptor_id} de {from_time} a {to_time} adicionados.\")\n",
        "            last_successful_date = current_date  # Atualiza a última data bem-sucedida\n",
        "        elif response.status_code == 404:\n",
        "            print(f\"Falha 404 na solicitação para metricDescriptorId {metric_descriptor_id} de {from_time} a {to_time}. Interrompendo a coleta para este ID.\")\n",
        "            break  # Interrompe o loop ao encontrar um erro 404\n",
        "        else:\n",
        "            print(f\"Falha na solicitação para metricDescriptorId {metric_descriptor_id} de {from_time} a {to_time}: {response.status_code}, {response.text}\")\n",
        "\n",
        "        # Incrementa a data atual para o próximo dia\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    # Armazena a última data bem-sucedida no DataFrame de controle\n",
        "    df_metric_descriptor_id_end_date = pd.concat([df_metric_descriptor_id_end_date, pd.DataFrame({\"metric_descriptor_id\": [metric_descriptor_id], \"last_end_date\": [last_successful_date.strftime(\"%Y-%m-%d\")]})], ignore_index=True)\n",
        "\n",
        "# Após o loop, salvar o DataFrame agregado como um único arquivo CSV\n",
        "output_filename = \"df_dynamoxAllData.csv\"\n",
        "csv_content = df_dynamoxAllData.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)\n",
        "\n",
        "# Salvar o DataFrame df_metric_descriptor_id_end_date como um novo CSV e fazer upload para o GitHub\n",
        "end_date_filename = \"df_metric_descriptor_id_end_date.csv\"\n",
        "end_date_content = df_metric_descriptor_id_end_date.to_csv(index=False)\n",
        "upload_csv_to_github(end_date_content, GITHUB_REPO, end_date_filename, GITHUB_TOKEN)\n",
        "\n"
      ],
      "metadata": {
        "id": "BX4kjpp2p9KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "import io\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Configurações do GitHub e do BigQuery\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "TOKEN_PATH = \"token.json\"  # Caminho do token.json no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "url_data_points = \"https://api.dynamox.solutions/v1/telemetry/data-points/aggregation\"\n",
        "\n",
        "# Configurações do BigQuery\n",
        "PROJECT_ID = \"rnn-component-life-cycle\"  # Substitua pelo ID do seu projeto no Google Cloud\n",
        "DATASET_ID = \"rnn-component-life-cycle.dmadymanox\"  # Nome do conjunto de dados no BigQuery\n",
        "TABLE_ID = \"dmadymanox\"  # Nome da tabela no BigQuery\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Carrega o access_token do token.json no GitHub\n",
        "token_data = json.loads(load_github_file(GITHUB_REPO, TOKEN_PATH, GITHUB_TOKEN))\n",
        "if token_data:\n",
        "    token = token_data.get(\"access_token\")\n",
        "    if not token:\n",
        "        raise ValueError(\"Access token não encontrado em token.json.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o token JSON do GitHub.\")\n",
        "\n",
        "# Carregar o arquivo metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Definindo o cabeçalho de autorização com o token\n",
        "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "# Definindo as datas de início e término para consulta diária\n",
        "start_date = datetime.strptime(\"2024-09-03\", \"%Y-%m-%d\")\n",
        "end_date = datetime.now()  # Data atual como data final\n",
        "\n",
        "# DataFrame agregado para todas as consultas\n",
        "df_dynamoxAllData = pd.DataFrame()\n",
        "\n",
        "# Loop sobre cada metricDescriptorId no DataFrame de métricas\n",
        "for metric_descriptor_id in metrics_df[\"metricDescriptorId\"]:\n",
        "    current_date = start_date\n",
        "    last_successful_date = start_date  # Inicializa a última data bem-sucedida com a data de início\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        # Configuração das datas para consulta diária\n",
        "        from_time = current_date.strftime(\"%Y-%m-%dT00:00:00.000Z\")\n",
        "        to_time = (current_date + timedelta(days=1)).strftime(\"%Y-%m-%dT00:00:00.000Z\")\n",
        "\n",
        "        # Parâmetros da consulta para cada ID de métrica com intervalo diário\n",
        "        params = {\n",
        "            \"metricDescriptorId\": metric_descriptor_id,\n",
        "            \"fromTime\": from_time,\n",
        "            \"toTime\": to_time,\n",
        "        }\n",
        "\n",
        "        # Fazendo a requisição para a API Dynamox\n",
        "        response = requests.get(url_data_points, headers=headers, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Processando os dados de resposta\n",
        "            data_points = response.json()\n",
        "            if data_points:\n",
        "                df = pd.DataFrame(data_points)\n",
        "\n",
        "                # Expandindo a coluna de agregação\n",
        "                aggregation_expanded = pd.json_normalize(df['aggregation'])\n",
        "\n",
        "                # Concatenando as colunas expandidas ao DataFrame original\n",
        "                df_expanded = pd.concat([df.drop(columns=['aggregation']), aggregation_expanded], axis=1)\n",
        "\n",
        "                # Adicionando uma coluna para identificar o metricDescriptorId\n",
        "                df_expanded['metricDescriptorId'] = metric_descriptor_id\n",
        "\n",
        "                # Agregando ao DataFrame principal\n",
        "                df_dynamoxAllData = pd.concat([df_dynamoxAllData, df_expanded], ignore_index=True)\n",
        "\n",
        "            print(f\"Dados agregados para metricDescriptorId {metric_descriptor_id} de {from_time} a {to_time} adicionados.\")\n",
        "            last_successful_date = current_date  # Atualiza a última data bem-sucedida\n",
        "        elif response.status_code == 404:\n",
        "            print(f\"Falha 404 na solicitação para metricDescriptorId {metric_descriptor_id} de {from_time} a {to_time}. Interrompendo a coleta para este ID.\")\n",
        "            break  # Interrompe o loop ao encontrar um erro 404\n",
        "        else:\n",
        "            print(f\"Falha na solicitação para metricDescriptorId {metric_descriptor_id} de {from_time} a {to_time}: {response.status_code}, {response.text}\")\n",
        "\n",
        "        # Incrementa a data atual para o próximo dia\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "# Função para salvar o DataFrame no Google BigQuery\n",
        "def save_to_bigquery(df, project_id, dataset_id, table_id):\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "    # Carregar os dados para o BigQuery\n",
        "    job = client.load_table_from_dataframe(df, table_ref)\n",
        "    job.result()  # Aguarda o término do job\n",
        "\n",
        "    print(f\"Dados salvos com sucesso no BigQuery na tabela {table_ref}\")\n",
        "\n",
        "# Salvar os dados agregados no BigQuery\n",
        "save_to_bigquery(df_dynamoxAllData, PROJECT_ID, DATASET_ID, TABLE_ID)\n"
      ],
      "metadata": {
        "id": "oChMSG5p3249"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ETL: Etapa 1.1) Ajuste no df_dynamox com ETL e criação do df_dynamoxnew, com colunas de descrição dos corretos Spots"
      ],
      "metadata": {
        "id": "D-YgBLV_MPUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import io\n",
        "from datetime import datetime\n",
        "from ast import literal_eval\n",
        "import re\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "DF_DYNAMOX_PATH = \"df_dynamox.csv\"  # Caminho do df_dynamox.csv no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de {path} - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo {path} salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carregar df_dynamox.csv do GitHub\n",
        "df_dynamox_content = load_github_file(GITHUB_REPO, DF_DYNAMOX_PATH, GITHUB_TOKEN)\n",
        "if df_dynamox_content:\n",
        "    df_dynamox = pd.read_csv(io.StringIO(df_dynamox_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo df_dynamox.csv do GitHub.\")\n",
        "\n",
        "# Carregar metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Fazendo o join para adicionar a coluna 'attributes' ao novo DataFrame df_dynamoxnew\n",
        "df_dynamoxnew = df_dynamox.merge(metrics_df[['metricDescriptorId', 'attributes']], on='metricDescriptorId', how='left')\n",
        "\n",
        "# Função para processar a coluna 'attributes' e converter em colunas separadas\n",
        "def parse_attributes(attribute_str):\n",
        "    try:\n",
        "        # Converte a string para um dicionário\n",
        "        attributes_dict = literal_eval(attribute_str)\n",
        "        return pd.Series(attributes_dict)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # Caso a string não possa ser convertida diretamente, faça uma separação manual\n",
        "        items = re.findall(r\"'([^']+)'\\s*:\\s*'([^']*)'\", attribute_str)\n",
        "        return pd.Series({k: v for k, v in items})\n",
        "\n",
        "# Verificar se a coluna 'attributes' existe e expandi-la em colunas separadas\n",
        "if 'attributes' in df_dynamoxnew.columns:\n",
        "    attributes_expanded = df_dynamoxnew['attributes'].apply(parse_attributes)\n",
        "    # Concatenar as novas colunas ao DataFrame original, criando o novo df_dynamoxnew\n",
        "    df_dynamoxnew = pd.concat([df_dynamoxnew.drop(columns=['attributes']), attributes_expanded], axis=1)\n",
        "\n",
        "# Criar a nova coluna combinada 'parametro' a partir de 'physicalQuantity' e 'axis'\n",
        "if 'physicalQuantity' in df_dynamoxnew.columns and 'axis' in df_dynamoxnew.columns:\n",
        "    df_dynamoxnew['parametro'] = df_dynamoxnew.apply(\n",
        "        lambda row: f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\") + \"\" if row['physicalQuantity'] == \"temperature\" else f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\"),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Remover as colunas desnecessárias após a concatenação\n",
        "df_dynamoxnew = df_dynamoxnew.drop(columns=['physicalQuantity', 'axis', 'source', 'statisticalProcessing'], errors='ignore')\n",
        "\n",
        "# Salvar o DataFrame atualizado como df_dynamoxnew.csv\n",
        "output_filename = \"df_dynamoxnew.csv\"\n",
        "csv_content = df_dynamoxnew.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF_yTtCpR_aj",
        "outputId": "b98768e1-89ce-4923-d13b-30aa68d0d536"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo df_dynamoxnew.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ETL: Etapa 1.2)  Criação do df_dynamoxnew, com o Join com o arquivo spotid (identificação/nomes dos Spots) e metricas"
      ],
      "metadata": {
        "id": "gzscdJ0gMad2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import io\n",
        "from datetime import datetime\n",
        "from ast import literal_eval\n",
        "import re\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "DF_DYNAMOX_PATH = \"df_dynamox.csv\"  # Caminho do df_dynamox.csv no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "SPOTID_PATH = \"spotid.csv\"  # Caminho do spotid.csv no repositório\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de {path} - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo {path} salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carregar df_dynamox.csv do GitHub\n",
        "df_dynamox_content = load_github_file(GITHUB_REPO, DF_DYNAMOX_PATH, GITHUB_TOKEN)\n",
        "if df_dynamox_content:\n",
        "    df_dynamox = pd.read_csv(io.StringIO(df_dynamox_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo df_dynamox.csv do GitHub.\")\n",
        "\n",
        "# Carregar metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Carregar spotid.csv do GitHub\n",
        "spotid_content = load_github_file(GITHUB_REPO, SPOTID_PATH, GITHUB_TOKEN)\n",
        "if spotid_content:\n",
        "    spotid_df = pd.read_csv(io.StringIO(spotid_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo spotid.csv do GitHub.\")\n",
        "\n",
        "# Fazendo o join para adicionar a coluna 'desc' como 'description' ao metrics_df\n",
        "metrics_with_desc = metrics_df.merge(spotid_df[['spotid', 'desc']], left_on='resourceId', right_on='spotid', how='left')\n",
        "metrics_with_desc = metrics_with_desc.rename(columns={'desc': 'description'}).drop(columns=['spotid'])\n",
        "\n",
        "# Fazendo o join para adicionar a coluna 'attributes' ao novo DataFrame df_dynamoxnew\n",
        "df_dynamoxnew = df_dynamox.merge(metrics_with_desc[['metricDescriptorId', 'attributes', 'description']], on='metricDescriptorId', how='left')\n",
        "\n",
        "# Função para processar a coluna 'attributes' e converter em colunas separadas\n",
        "def parse_attributes(attribute_str):\n",
        "    try:\n",
        "        # Converte a string para um dicionário\n",
        "        attributes_dict = literal_eval(attribute_str)\n",
        "        return pd.Series(attributes_dict)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # Caso a string não possa ser convertida diretamente, faça uma separação manual\n",
        "        items = re.findall(r\"'([^']+)'\\s*:\\s*'([^']*)'\", attribute_str)\n",
        "        return pd.Series({k: v for k, v in items})\n",
        "\n",
        "# Verificar se a coluna 'attributes' existe e expandi-la em colunas separadas\n",
        "if 'attributes' in df_dynamoxnew.columns:\n",
        "    attributes_expanded = df_dynamoxnew['attributes'].apply(parse_attributes)\n",
        "    # Concatenar as novas colunas ao DataFrame original, criando o novo df_dynamoxnew\n",
        "    df_dynamoxnew = pd.concat([df_dynamoxnew.drop(columns=['attributes']), attributes_expanded], axis=1)\n",
        "\n",
        "# Criar a nova coluna combinada 'parametro' a partir de 'physicalQuantity' e 'axis'\n",
        "if 'physicalQuantity' in df_dynamoxnew.columns and 'axis' in df_dynamoxnew.columns:\n",
        "    df_dynamoxnew['parametro'] = df_dynamoxnew.apply(\n",
        "        lambda row: f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\") + \"\" if row['physicalQuantity'] == \"temperature\" else f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\"),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Remover as colunas desnecessárias após a concatenação\n",
        "df_dynamoxnew = df_dynamoxnew.drop(columns=['physicalQuantity', 'axis', 'source', 'statisticalProcessing'], errors='ignore')\n",
        "\n",
        "# Salvar o DataFrame atualizado como df_dynamoxnew.csv\n",
        "output_filename = \"df_dynamoxnew.csv\"\n",
        "csv_content = df_dynamoxnew.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ckT0aITBhW4",
        "outputId": "56637716-159b-4f95-ff9e-87b0d811c3df"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo df_dynamoxnew.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ETL 1.2.3 Remoção Coluna \"metricDescriptorId\" do df, e criação de novo df_DynamoxAcm"
      ],
      "metadata": {
        "id": "gxjlg7W_cigL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "DF_DYNAMOXNEW_PATH = \"df_dynamoxnew.csv\"  # Caminho do df_dynamoxnew.csv no repositório\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Carregar df_dynamoxnew.csv do GitHub\n",
        "df_dynamoxnew_content = load_github_file(GITHUB_REPO, DF_DYNAMOXNEW_PATH, GITHUB_TOKEN)\n",
        "if df_dynamoxnew_content:\n",
        "    df_dynamoxnew = pd.read_csv(io.StringIO(df_dynamoxnew_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo df_dynamoxnew.csv do GitHub.\")\n",
        "\n",
        "# Criar o novo DataFrame df_DynamoxAcm (personalize a transformação conforme necessário)\n",
        "# Exemplo simples: Vamos copiar o DataFrame df_dynamoxnew para df_DynamoxAcm\n",
        "df_DynamoxAcm = df_dynamoxnew.copy()\n",
        "\n",
        "# Exemplo de ajuste adicional: filtrar linhas ou adicionar colunas, conforme necessário\n",
        "# Aqui, vamos apenas ajustar a coluna datetime se estiver presente\n",
        "if 'datetime' in df_DynamoxAcm.columns:\n",
        "    df_DynamoxAcm['datetime'] = pd.to_datetime(df_DynamoxAcm['datetime'], utc=True).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(\"A coluna 'datetime' foi ajustada para o formato 'YYYY-MM-DD HH:MM:SS'.\")\n",
        "\n",
        "# Exibir o novo DataFrame para verificação\n",
        "print(df_DynamoxAcm.head())\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de {path} - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo {path} salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Salvar o novo DataFrame df_DynamoxAcm como df_DynamoxAcm.csv e fazer upload para o GitHub\n",
        "output_filename = \"df_DynamoxAcm.csv\"\n",
        "csv_content = df_DynamoxAcm.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvo574kIaRTE",
        "outputId": "515735d5-76c8-4152-8eab-f582df5df71c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A coluna 'datetime' foi ajustada para o formato 'YYYY-MM-DD HH:MM:SS'.\n",
            "         metricDescriptorId             datetime        max       avg  \\\n",
            "0  668febb557480af249f91a86  2024-09-03 22:31:50  13.096000  3.340844   \n",
            "1  668febb557480af249f91a86  2024-09-04 00:01:50  51.101559  8.503232   \n",
            "2  668febb557480af249f91a86  2024-09-06 00:01:50  34.036518  8.231985   \n",
            "3  668febb557480af249f91a86  2024-09-08 00:01:50  24.390097  8.423476   \n",
            "4  668febb557480af249f91a86  2024-09-10 00:01:50  27.148045  9.026620   \n",
            "\n",
            "        min  count             description   parametro  \n",
            "0  0.255058      9  EM3401 Bomba Secção P1  velocity_x  \n",
            "1  0.129316    288  EM3401 Bomba Secção P1  velocity_x  \n",
            "2  0.102631    288  EM3401 Bomba Secção P1  velocity_x  \n",
            "3  0.168550    288  EM3401 Bomba Secção P1  velocity_x  \n",
            "4  0.117511    282  EM3401 Bomba Secção P1  velocity_x  \n",
            "Arquivo df_DynamoxAcm.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "name": "api_dynamox.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}