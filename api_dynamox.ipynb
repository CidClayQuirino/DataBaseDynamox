{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CidClayQuirino/DataBaseDynamox/blob/main/api_dynamox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyG45Qk3qQLS"
      },
      "source": [
        "# API - Dynamox\n",
        "Upload dass bases de dados Dynamox do GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR921S_OQSHG"
      },
      "source": [
        "## Package install & Update\n",
        "A seguir estão os pacotes requeridos para tratamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "id": "WUtu4316QSHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbb1712-f53b-4978-a949-7d1277d6b59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/375.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/375.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.9/375.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/856.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install markdown -q\n",
        "%pip install PyGithub -q\n",
        "%pip install gitpython -q\n",
        "%pip install openpyxl -q\n",
        "%pip install requests -q\n",
        "\n",
        "# Atualizar pacotes\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_TOKEN = \"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\""
      ],
      "metadata": {
        "id": "cLiWUP7o6hCm"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geração do Token e API para download dos dados da plataforma Dynamox"
      ],
      "metadata": {
        "id": "hAt1QCnpk4ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # pip install requests\n",
        "import jwt  # pip install pyjwt\n",
        "import base64\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# URLs e caminho para o JSON e para o GitHub\n",
        "APPLICATION_KEY_URL = \"https://raw.githubusercontent.com/CidClayQuirino/DataBaseDynamox/main/SOTREQ.json\"\n",
        "AUTH_URL = \"https://api.dynamox.solutions/v1/token\"  # Substitua pelo endpoint que retorna o access_token\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "GITHUB_PATH = \"token.json\"  # Caminho no repositório onde o token será salvo\n",
        "\n",
        "# Função para baixar o JSON diretamente da URL\n",
        "def load_json_from_url(url: str) -> dict:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(\"Erro ao baixar o JSON:\", response.status_code)\n",
        "        exit()\n",
        "\n",
        "# Função para gerar o token JWT\n",
        "def generate_token(data: dict) -> str:\n",
        "    return jwt.encode(\n",
        "        headers={\n",
        "            \"kid\": data[\"_id\"],\n",
        "            \"alg\": \"RS256\",\n",
        "            \"typ\": \"JWT\",\n",
        "        },\n",
        "        payload={\n",
        "            \"iat\": datetime.now(timezone.utc).timestamp(),\n",
        "            \"email\": data[\"email\"],\n",
        "        },\n",
        "        key=data[\"privateKey\"].encode(\"utf-8\"),\n",
        "        algorithm=\"RS256\"\n",
        "    )\n",
        "\n",
        "# Função para solicitar o access_token usando o token JWT\n",
        "def get_access_token(jwt_token: str) -> str:\n",
        "    headers = {\"Authorization\": f\"Bearer {jwt_token}\"}\n",
        "    response = requests.post(url=AUTH_URL, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"access_token\")\n",
        "    else:\n",
        "        print(\"Erro ao obter o access_token:\", response.status_code, response.text)\n",
        "        exit()\n",
        "\n",
        "# Função para fazer upload do JSON com o token para o GitHub\n",
        "def upload_to_github(json_content: str, repo: str, path: str, github_token: str):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(json_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": \"Upload do token de autenticação em JSON\",\n",
        "        \"content\": content\n",
        "    }\n",
        "    # Verifica se o arquivo já existe para obter o SHA e fazer o update\n",
        "    get_response = requests.get(url, headers=headers)\n",
        "    if get_response.status_code == 200:\n",
        "        sha = get_response.json()[\"sha\"]\n",
        "        data[\"sha\"] = sha\n",
        "\n",
        "    # Faz o upload ou update do arquivo no GitHub\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(\"Token JSON salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(\"Erro ao salvar o token JSON no GitHub:\", response.status_code, response.text)\n",
        "\n",
        "# Carrega o JSON da URL\n",
        "json_data = load_json_from_url(APPLICATION_KEY_URL)\n",
        "\n",
        "# Gera o token JWT usando os dados do JSON\n",
        "jwt_token = generate_token(json_data)\n",
        "print(\"Token JWT gerado:\", jwt_token)\n",
        "\n",
        "# Obtém o access_token usando o token JWT\n",
        "access_token = get_access_token(jwt_token)\n",
        "print(\"Access token obtido:\", access_token)\n",
        "\n",
        "# Salva o access_token em JSON e faz upload para o GitHub\n",
        "token_data = {\"access_token\": access_token}\n",
        "token_json = json.dumps(token_data, indent=4)\n",
        "upload_to_github(token_json, GITHUB_REPO, GITHUB_PATH, GITHUB_TOKEN)"
      ],
      "metadata": {
        "id": "HfWCKz3nlB_k",
        "outputId": "f3d619ae-e34c-40e1-9bcf-93b419c88a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token JWT gerado: eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3MjhmN2Q4NTZkMTJkNzI2MGVhMzU0MyIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzExNTYxNTQuMTE4MDQxLCJlbWFpbCI6IjY3MjhmN2JkNWZiYzllM2YzZTE4Yzg1M0BzYS5keW5hbW94LnNvbHV0aW9ucyJ9.cmZWb5b-7SB9Oik25LkAIBkdBQEvcpYGcld30KFFlFAecUiea_1HWkiRxIsB1dz-CpLfDWXvjZaxaRotH77VDu-KYOlfTIlXeS91eXY9lU9LA_xG0uUm12s7bVqKxfS_9AIKkwqwOP1xyWMkCyQQyC6579zT3KQS_0XJePGeMN978SH7oLk1mJHq3-mhmKSgqK6KJCNOnadoQ0Off-La5rvvDRjA9vzS8RrCFzoa1dhnBBnNCjREiNNP-JdHG-fiCMW8ikgoaVAazDhkQl6XLle3nmJbgi5PL5-S_CgPLh2AEFMy_kabfaL13GzyTqcQIvtc-75OJLSp7tgUyfvz4Q\n",
            "Access token obtido: eyJhbGciOiJSUzI1NiIsImtpZCI6ImI4Y2FjOTViNGE1YWNkZTBiOTY1NzJkZWU4YzhjOTVlZWU0OGNjY2QiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiU09UUkVRIFMvQSAtIE1haW4gYXBwbGljYXRpb24iLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vcHJlZGljdC1wcm9kdWN0aW9uLXBsYXRmb3JtIiwiYXVkIjoicHJlZGljdC1wcm9kdWN0aW9uLXBsYXRmb3JtIiwiYXV0aF90aW1lIjoxNzMxMTU2MTU0LCJ1c2VyX2lkIjoia1YyVU5hRE5ROFZYMXJjM0hPYlZnU29pTG11MSIsInN1YiI6ImtWMlVOYUROUThWWDFyYzNIT2JWZ1NvaUxtdTEiLCJpYXQiOjE3MzExNTYxNTQsImV4cCI6MTczMTE1OTc1NCwiZW1haWwiOiI2NzI4ZjdiZDVmYmM5ZTNmM2UxOGM4NTNAc2EuZHluYW1veC5zb2x1dGlvbnMiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZW1haWwiOlsiNjcyOGY3YmQ1ZmJjOWUzZjNlMThjODUzQHNhLmR5bmFtb3guc29sdXRpb25zIl19LCJzaWduX2luX3Byb3ZpZGVyIjoicGFzc3dvcmQifX0.Va-hd2ocnFFQpuNTWI3BWX_omAPaMXwuHzwT5yix_ZrKlMJ-tOztnyXUDanrCQ-J2d6l-GqAXQcKSm6PKmP7iBriTag9-p3DDaN8b0eql3QNZnkmesRKk1V--paP9T594uOtuZWsqI8LQQEf6YAIKZCnZhQr9P-v_OSbbU7CAjLBEx8o-BNkWB5yckX_TxSVOXTwVInbt1zmZm-4a74uVB4MDl-IlVLj7RY5sKttd3k87zzX8a8Ik0ZtM7tEGBMtLZt17ry7MtmmmkmYr3V0o3C8hEphrAGpE0uw_qruBihVsPbSGB0EdLGysHP-qfwSI-5itpmaY0XM0gViAO3evA\n",
            "Token JSON salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurações do GitHub e URLs\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "GITHUB_PATH = \"token.json\"  # Caminho do token.json no repositório\n",
        "CSV_PATH_ON_GITHUB = \"metrics_data.csv\"\n",
        "url_metric_descriptor = \"https://api.dynamox.solutions/v1/telemetry/metric-descriptor\"\n",
        "resource_ids = [\n",
        "    \"668febb5b059577b39e5fa09\", \"668febb2a9700b05a71ca37d\", \"668ff6e6765a3c089d872db8\",\n",
        "    \"668ff6e9a9700bbff21ca91c\", \"668ff96083c61336dd517c5d\", \"66cf825924e3d86b465ec589\",\n",
        "    \"667ed1804d13425c5387a272\", \"668fe7e938e2532db463d8b6\", \"668fe7eba9700bf2361ca195\",\n",
        "    \"66ddd667ea40c41c2eee6f05\", \"66d70a35a6119ffc0cd30d32\", \"668febadb05957ae16e5fa05\", \"66ddda0b13eff3dbf760a767\"\n",
        "]\n",
        "\n",
        "# Função para baixar e carregar o token JSON do repositório GitHub\n",
        "def load_github_token(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return json.loads(content)\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Carrega o access_token do token.json no GitHub\n",
        "token_data = load_github_token(GITHUB_REPO, GITHUB_PATH, GITHUB_TOKEN)\n",
        "if token_data:\n",
        "    access_token = token_data.get(\"access_token\")  # Substitua \"access_token\" pela chave exata dentro do token.json\n",
        "    if not access_token:\n",
        "        raise ValueError(\"Access token não encontrado em token.json.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o token JSON do GitHub.\")\n",
        "\n",
        "# Cabeçalho de autorização com o token JWT\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "\n",
        "# Consulta os descritores de métricas para cada resource_id\n",
        "all_metrics = []\n",
        "for resource_id in resource_ids:\n",
        "    params = {\"resourceId\": resource_id}\n",
        "    response = requests.get(url_metric_descriptor, headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        metric_descriptors = response.json()\n",
        "        all_metrics.extend(metric_descriptors)\n",
        "    else:\n",
        "        print(f\"Falha na solicitação para resource_id {resource_id}: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Cria um DataFrame e salva como CSV\n",
        "df = pd.DataFrame(all_metrics)\n",
        "csv_data = df.to_csv(index=False)\n",
        "\n",
        "# Função para fazer upload do CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content: str, repo: str, path: str, github_token: str):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de dados de métricas - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "    # Verifica se o arquivo já existe para atualização\n",
        "    get_response = requests.get(url, headers=headers)\n",
        "    if get_response.status_code == 200:\n",
        "        sha = get_response.json()[\"sha\"]\n",
        "        data[\"sha\"] = sha\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(\"CSV salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o CSV no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Upload do CSV para o GitHub\n",
        "upload_csv_to_github(csv_data, GITHUB_REPO, CSV_PATH_ON_GITHUB, GITHUB_TOKEN)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUXOZqrdAjVf",
        "outputId": "6da5cf25-a4bd-4481-9415-d53c1b4773ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime\n",
        "import io\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "TOKEN_PATH = \"token.json\"  # Caminho do token.json no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "url_data_points = \"https://api.dynamox.solutions/v1/telemetry/data-points/aggregation\"\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    # Encode the content to base64\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de dados agregados - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo df_dynamox.csv salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carrega o access_token do token.json no GitHub\n",
        "token_data = json.loads(load_github_file(GITHUB_REPO, TOKEN_PATH, GITHUB_TOKEN))\n",
        "if token_data:\n",
        "    token = token_data.get(\"access_token\")\n",
        "    if not token:\n",
        "        raise ValueError(\"Access token não encontrado em token.json.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o token JSON do GitHub.\")\n",
        "\n",
        "# Carregar o arquivo metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Definindo o cabeçalho de autorização com o token\n",
        "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "# Período de consulta\n",
        "from_time = \"2024-01-01\"\n",
        "to_time = \"2024-12-30\"\n",
        "\n",
        "# DataFrame agregado para todas as métricas\n",
        "df_dynamox = pd.DataFrame()\n",
        "\n",
        "# Loop sobre cada metricDescriptorId no DataFrame de métricas\n",
        "for metric_descriptor_id in metrics_df[\"metricDescriptorId\"]:\n",
        "    # Parâmetros da consulta para cada ID de métrica\n",
        "    params = {\n",
        "        \"metricDescriptorId\": metric_descriptor_id,\n",
        "        \"fromTime\": f\"{from_time}T12:33:11.123Z\",\n",
        "        \"toTime\": f\"{to_time}T12:33:11.123Z\",\n",
        "    }\n",
        "\n",
        "    # Fazendo a requisição para a API Dynamox\n",
        "    response = requests.get(url_data_points, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Processando os dados de resposta\n",
        "        data_points = response.json()\n",
        "        df = pd.DataFrame(data_points)\n",
        "\n",
        "        # Expandindo a coluna de agregação\n",
        "        aggregation_expanded = pd.json_normalize(df['aggregation'])\n",
        "\n",
        "        # Concatenando as colunas expandidas ao DataFrame original\n",
        "        df_expanded = pd.concat([df.drop(columns=['aggregation']), aggregation_expanded], axis=1)\n",
        "\n",
        "        # Adicionando uma coluna para identificar o metricDescriptorId\n",
        "        df_expanded['metricDescriptorId'] = metric_descriptor_id\n",
        "\n",
        "        # Agregando ao DataFrame principal\n",
        "        df_dynamox = pd.concat([df_dynamox, df_expanded], ignore_index=True)\n",
        "\n",
        "    else:\n",
        "        print(f\"Falha na solicitação para metricDescriptorId {metric_descriptor_id}: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Após o loop, salvar o DataFrame agregado como um único arquivo CSV\n",
        "output_filename = \"df_dynamox.csv\"\n",
        "csv_content = df_dynamox.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7zDM2zSJxl5",
        "outputId": "44ec54a0-3c16-4234-8d06-9a3e8c209340"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd5458: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd5458\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd546e: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd546e\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd5479: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd5479\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd548f: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd548f\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd54ab: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd54ab\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd54bf: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd54bf\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Falha na solicitação para metricDescriptorId 667ed180542fd60b4ffd54d6: 404, {\"detail\":{\"message\":\"There are no Data Points for aggregation.\",\"reason\":{\"resourceId\":\"667ed1804d13425c5387a272\",\"metricDescriptorId\":\"667ed180542fd60b4ffd54d6\",\"from_time\":\"2024-01-01T12:33:11.123Z\",\"to_time\":\"2024-12-30T12:33:11.123Z\"}}}\n",
            "Arquivo df_dynamox.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import io\n",
        "from datetime import datetime\n",
        "from ast import literal_eval\n",
        "import re\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "DF_DYNAMOX_PATH = \"df_dynamox.csv\"  # Caminho do df_dynamox.csv no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de {path} - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo {path} salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carregar df_dynamox.csv do GitHub\n",
        "df_dynamox_content = load_github_file(GITHUB_REPO, DF_DYNAMOX_PATH, GITHUB_TOKEN)\n",
        "if df_dynamox_content:\n",
        "    df_dynamox = pd.read_csv(io.StringIO(df_dynamox_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo df_dynamox.csv do GitHub.\")\n",
        "\n",
        "# Carregar metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Fazendo o join para adicionar a coluna 'attributes' ao novo DataFrame df_dynamoxnew\n",
        "df_dynamoxnew = df_dynamox.merge(metrics_df[['metricDescriptorId', 'attributes']], on='metricDescriptorId', how='left')\n",
        "\n",
        "# Função para processar a coluna 'attributes' e converter em colunas separadas\n",
        "def parse_attributes(attribute_str):\n",
        "    try:\n",
        "        # Converte a string para um dicionário\n",
        "        attributes_dict = literal_eval(attribute_str)\n",
        "        return pd.Series(attributes_dict)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # Caso a string não possa ser convertida diretamente, faça uma separação manual\n",
        "        items = re.findall(r\"'([^']+)'\\s*:\\s*'([^']*)'\", attribute_str)\n",
        "        return pd.Series({k: v for k, v in items})\n",
        "\n",
        "# Verificar se a coluna 'attributes' existe e expandi-la em colunas separadas\n",
        "if 'attributes' in df_dynamoxnew.columns:\n",
        "    attributes_expanded = df_dynamoxnew['attributes'].apply(parse_attributes)\n",
        "    # Concatenar as novas colunas ao DataFrame original, criando o novo df_dynamoxnew\n",
        "    df_dynamoxnew = pd.concat([df_dynamoxnew.drop(columns=['attributes']), attributes_expanded], axis=1)\n",
        "\n",
        "# Criar a nova coluna combinada 'parametro' a partir de 'physicalQuantity' e 'axis'\n",
        "if 'physicalQuantity' in df_dynamoxnew.columns and 'axis' in df_dynamoxnew.columns:\n",
        "    df_dynamoxnew['parametro'] = df_dynamoxnew.apply(\n",
        "        lambda row: f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\") + \"\" if row['physicalQuantity'] == \"temperature\" else f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\"),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Remover as colunas desnecessárias após a concatenação\n",
        "df_dynamoxnew = df_dynamoxnew.drop(columns=['physicalQuantity', 'axis', 'source', 'statisticalProcessing'], errors='ignore')\n",
        "\n",
        "# Salvar o DataFrame atualizado como df_dynamoxnew.csv\n",
        "output_filename = \"df_dynamoxnew.csv\"\n",
        "csv_content = df_dynamoxnew.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF_yTtCpR_aj",
        "outputId": "ee47466b-7052-49a8-abdd-1969c917414a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquivo df_dynamoxnew.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import io\n",
        "from datetime import datetime\n",
        "from ast import literal_eval\n",
        "import re\n",
        "\n",
        "# Configurações do GitHub\n",
        "GITHUB_REPO = \"CidClayQuirino/DataBaseDynamox\"\n",
        "DF_DYNAMOX_PATH = \"df_dynamox.csv\"  # Caminho do df_dynamox.csv no repositório\n",
        "METRICS_DATA_PATH = \"metrics_data.csv\"  # Caminho do metrics_data.csv no repositório\n",
        "SPOTID_PATH = \"spotid.csv\"  # Caminho do spotid.csv no repositório\n",
        "\n",
        "# Função para carregar um arquivo do GitHub\n",
        "def load_github_file(repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        content = base64.b64decode(response.json()[\"content\"]).decode()\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Erro ao acessar {path} no GitHub: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Função para fazer upload de um arquivo CSV para o GitHub\n",
        "def upload_csv_to_github(csv_content, repo, path, github_token):\n",
        "    url = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {github_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    content = base64.b64encode(csv_content.encode()).decode()\n",
        "    data = {\n",
        "        \"message\": f\"Upload de {path} - {datetime.now().isoformat()}\",\n",
        "        \"content\": content\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo já existe para atualizar\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        sha = response.json().get(\"sha\")\n",
        "        data[\"sha\"] = sha  # Necessário para atualização de arquivo existente\n",
        "\n",
        "    # Faz o upload ou atualização do CSV\n",
        "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code in [200, 201]:\n",
        "        print(f\"Arquivo {path} salvo com sucesso no GitHub.\")\n",
        "    else:\n",
        "        print(f\"Erro ao salvar o arquivo no GitHub: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Carregar df_dynamox.csv do GitHub\n",
        "df_dynamox_content = load_github_file(GITHUB_REPO, DF_DYNAMOX_PATH, GITHUB_TOKEN)\n",
        "if df_dynamox_content:\n",
        "    df_dynamox = pd.read_csv(io.StringIO(df_dynamox_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo df_dynamox.csv do GitHub.\")\n",
        "\n",
        "# Carregar metrics_data.csv do GitHub\n",
        "metrics_data_content = load_github_file(GITHUB_REPO, METRICS_DATA_PATH, GITHUB_TOKEN)\n",
        "if metrics_data_content:\n",
        "    metrics_df = pd.read_csv(io.StringIO(metrics_data_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo metrics_data.csv do GitHub.\")\n",
        "\n",
        "# Carregar spotid.csv do GitHub\n",
        "spotid_content = load_github_file(GITHUB_REPO, SPOTID_PATH, GITHUB_TOKEN)\n",
        "if spotid_content:\n",
        "    spotid_df = pd.read_csv(io.StringIO(spotid_content))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Não foi possível carregar o arquivo spotid.csv do GitHub.\")\n",
        "\n",
        "# Fazendo o join para adicionar a coluna 'desc' como 'description' ao metrics_df\n",
        "metrics_with_desc = metrics_df.merge(spotid_df[['spotid', 'desc']], left_on='resourceId', right_on='spotid', how='left')\n",
        "metrics_with_desc = metrics_with_desc.rename(columns={'desc': 'description'}).drop(columns=['spotid'])\n",
        "\n",
        "# Fazendo o join para adicionar a coluna 'attributes' ao novo DataFrame df_dynamoxnew\n",
        "df_dynamoxnew = df_dynamox.merge(metrics_with_desc[['metricDescriptorId', 'attributes', 'description']], on='metricDescriptorId', how='left')\n",
        "\n",
        "# Função para processar a coluna 'attributes' e converter em colunas separadas\n",
        "def parse_attributes(attribute_str):\n",
        "    try:\n",
        "        # Converte a string para um dicionário\n",
        "        attributes_dict = literal_eval(attribute_str)\n",
        "        return pd.Series(attributes_dict)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # Caso a string não possa ser convertida diretamente, faça uma separação manual\n",
        "        items = re.findall(r\"'([^']+)'\\s*:\\s*'([^']*)'\", attribute_str)\n",
        "        return pd.Series({k: v for k, v in items})\n",
        "\n",
        "# Verificar se a coluna 'attributes' existe e expandi-la em colunas separadas\n",
        "if 'attributes' in df_dynamoxnew.columns:\n",
        "    attributes_expanded = df_dynamoxnew['attributes'].apply(parse_attributes)\n",
        "    # Concatenar as novas colunas ao DataFrame original, criando o novo df_dynamoxnew\n",
        "    df_dynamoxnew = pd.concat([df_dynamoxnew.drop(columns=['attributes']), attributes_expanded], axis=1)\n",
        "\n",
        "# Criar a nova coluna combinada 'parametro' a partir de 'physicalQuantity' e 'axis'\n",
        "if 'physicalQuantity' in df_dynamoxnew.columns and 'axis' in df_dynamoxnew.columns:\n",
        "    df_dynamoxnew['parametro'] = df_dynamoxnew.apply(\n",
        "        lambda row: f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\") + \"\" if row['physicalQuantity'] == \"temperature\" else f\"{row['physicalQuantity']}_{row['axis']}\".replace(\"_nan\", \"\"),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Remover as colunas desnecessárias após a concatenação\n",
        "df_dynamoxnew = df_dynamoxnew.drop(columns=['physicalQuantity', 'axis', 'source', 'statisticalProcessing'], errors='ignore')\n",
        "\n",
        "# Salvar o DataFrame atualizado como df_dynamoxnew.csv\n",
        "output_filename = \"df_dynamoxnew.csv\"\n",
        "csv_content = df_dynamoxnew.to_csv(index=False)\n",
        "upload_csv_to_github(csv_content, GITHUB_REPO, output_filename, GITHUB_TOKEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ckT0aITBhW4",
        "outputId": "23112a79-5d81-4eef-bee5-76abfabfbe75"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo df_dynamoxnew.csv salvo com sucesso no GitHub.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "name": "api_dynamox.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}